\section{Introduction}
New advancements in technologies have resulted in the proliferation of data production at an unprecendented scale, leading scientists to the problem that the capacity to organize, let alone find or understand the data, has lagged behind our ability to generate data. Back in 2006, shortly after the popularization of next-generation sequencing, it was found that data volumnes were doubling every year and becoming increasingly complex and correlated over many dimensions~\cite{szalay20062020}. Using nucleotide sequencing as an example, it was found in 2009 that submission rates to a global database were growing at 200\% per annum; conservation of this data is guarded by three global repositories which exchange new data on a daily basis~\cite{southan2009beyond}.

Alongside this proliferation, where invested financial resources may be provided by the public sector, follows an expectation of freely accessible data for global biomedical research. As such, much of the scientific community has been concerned with the scalability and collaborative issues associated with data management like necessary standardization and infrastructure needed to support useful processing and workflow methods. Increased commitments to data sharing is reinforced by efforts dedicated to building databases, repositories and biobanks~\cite{leonelli2012introduction}. Multidisciplinary databases also led to the collection and combined analysis of data found from multiple repositories, which eventually may be deposited elsewhere for shared use. In a generation of data-intensive and data-driven computing, scientists are engaging in records as a corpus of text and collections of interlinked data resources that may identify papers of interest, suggest hypotheses to explore, and even the production of new data~\cite{lynch2009jim}.

Existing entities have been addressing issues to facilitate data use: the European Bioinformatics Institute has developed open standards and search systems indexing data, and the National Center for Biotechnology Information (NCBI) has invested greatly in the PubMed~\cite{pubmed} search engine for accessing biomedical literature citations of the MEDLINE database. Despite these efforts, the growing size and number of partially coordinated and incompatible data set formats makes data discovery and integration a notable challenge. For example, author analysis on Pubmed is driven by crawling, so manually converting this into structured data before analysis is a task in and of itself. But before even considering the issues of mining these vast troves of data, one needs to somehow identify these relevant data sets.

This paper proposes a way to augment the recommendation framework currently utilized in the DataRank~\cite{datarank} prototype which aims to rank datasets based on generated features and users' feedback. By exploring several ranking approaches on citation and authorship networks, publications and authors can be individually or synchronously ranked with their ranking values represented as a probability distribution. Datasets are associated with papers as a linear combination of weights, and a ranking metric is used to generate conditional probabilities for datasets given author.